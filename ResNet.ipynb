{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alimomennasab/ResNet/blob/master/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo7NbGZoYA22"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "fcpW0TZzoBRg"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "enUrZEz0GAio"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "gax1apR5GHpC"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "num_epochs = 80\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HWE-r36ctn_"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZODz0H8b70B",
        "outputId": "04383511-e7d1-4037-f05f-31c0c5078868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Image preprocessing modules\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "# CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                             train=True, \n",
        "                                             transform=transform,\n",
        "                                             download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                            train=False, \n",
        "                                            transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "0o4PGyR55HAf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjjGTp0e43AS"
      },
      "source": [
        "## Preparing Dataset & Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "0V5xaKH_4778"
      },
      "outputs": [],
      "source": [
        "# Image preprocessing modules\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "PfZPzna26FF7"
      },
      "outputs": [],
      "source": [
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=100, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=100, \n",
        "                                          shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shv6Es8j6RM7"
      },
      "source": [
        "## Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "c83-SoBb6TFd"
      },
      "outputs": [],
      "source": [
        "class block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride = 1):\n",
        "        super(block, self).__init__()\n",
        "        self.expansion = 4\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class ResNet(nn.Module): # [3, 4, 6, 3]\n",
        "    def __init__(self, block, layers, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        #ResNet layers\n",
        "        self.layer1 = self._make_layer(block, layers[0], out_channels=64, stride=1)\n",
        "        self.layer2 = self._make_layer(block, layers[1], out_channels=128, stride=2)\n",
        "        self.layer3 = self._make_layer(block, layers[2], out_channels=256, stride=2)\n",
        "        self.layer4 = self._make_layer(block, layers[3], out_channels=512, stride=2) #2048 channels at end\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512 * 4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, num_residual_blocks, out_channels, stride):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1 or self.in_channels != out_channels * 4:\n",
        "            identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels*4, kernel_size=1,\n",
        "                                                          stride=stride),\n",
        "                                                nn.BatchNorm2d(out_channels*4))\n",
        "\n",
        "        layers.append(block(self.in_channels, out_channels, identity_downsample, stride))\n",
        "        self.in_channels = out_channels*4 #256 in channels\n",
        "\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(self.in_channels, out_channels)) # 256 in channels, 64 out channels. 256 -> 64, 64*4 (256) again\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def ResNet50(img_channels=3, num_classes=1000):\n",
        "    return ResNet(block, [3, 4, 6, 3], img_channels, num_classes)\n",
        "\n",
        "def ResNet101(img_channels=3, num_classes=1000):\n",
        "    return ResNet(block, [3, 4, 23, 3], img_channels, num_classes)\n",
        "\n",
        "def ResNet152(img_channels=3, num_classes=1000):\n",
        "    return ResNet(block, [3, 8, 36, 3], img_channels, num_classes)\n",
        "\n",
        "#model = ResNet50()\n",
        "#model = ResNet101()\n",
        "model = ResNet152()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoAddyKfAmmp"
      },
      "source": [
        "## Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "JVUPmyy7AzRV"
      },
      "outputs": [],
      "source": [
        " #def test():\n",
        " #   net = ResNet152()\n",
        " #   x = torch.randn(2, 3, 224, 224)\n",
        " #   y = net(x).to('cpu') #change to cuda\n",
        " #   print(y.shape) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "9f3PVzuAA1Ty"
      },
      "outputs": [],
      "source": [
        "# test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hJFqp0sCl7_"
      },
      "source": [
        "## Defining Main Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3OhetHBCpKe",
        "outputId": "960c2e36-a46c-4a2e-9bd7-77b88b0dd336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/80], Step [100/500] Loss: 2.2986\n",
            "Epoch [1/80], Step [200/500] Loss: 2.0176\n",
            "Epoch [1/80], Step [300/500] Loss: 2.3025\n",
            "Epoch [1/80], Step [400/500] Loss: 1.7837\n",
            "Epoch [1/80], Step [500/500] Loss: 1.4769\n",
            "Epoch [2/80], Step [100/500] Loss: 2.2395\n",
            "Epoch [2/80], Step [200/500] Loss: 2.2706\n",
            "Epoch [2/80], Step [300/500] Loss: 1.4606\n",
            "Epoch [2/80], Step [400/500] Loss: 1.4995\n",
            "Epoch [2/80], Step [500/500] Loss: 1.7027\n",
            "Epoch [3/80], Step [100/500] Loss: 1.3255\n",
            "Epoch [3/80], Step [200/500] Loss: 1.5578\n",
            "Epoch [3/80], Step [300/500] Loss: 1.8925\n",
            "Epoch [3/80], Step [400/500] Loss: 1.4244\n",
            "Epoch [3/80], Step [500/500] Loss: 1.3564\n",
            "Epoch [4/80], Step [100/500] Loss: 1.3418\n",
            "Epoch [4/80], Step [200/500] Loss: 1.6297\n",
            "Epoch [4/80], Step [300/500] Loss: 1.5322\n",
            "Epoch [4/80], Step [400/500] Loss: 1.4135\n",
            "Epoch [4/80], Step [500/500] Loss: 1.4095\n",
            "Epoch [5/80], Step [100/500] Loss: 1.2314\n",
            "Epoch [5/80], Step [200/500] Loss: 1.4072\n",
            "Epoch [5/80], Step [300/500] Loss: 1.2419\n",
            "Epoch [5/80], Step [400/500] Loss: 1.9750\n",
            "Epoch [5/80], Step [500/500] Loss: 1.2645\n",
            "Epoch [6/80], Step [100/500] Loss: 1.8491\n",
            "Epoch [6/80], Step [200/500] Loss: 1.6768\n",
            "Epoch [6/80], Step [300/500] Loss: 2.2225\n",
            "Epoch [6/80], Step [400/500] Loss: 1.6115\n",
            "Epoch [6/80], Step [500/500] Loss: 1.5410\n",
            "Epoch [7/80], Step [100/500] Loss: 1.5414\n",
            "Epoch [7/80], Step [200/500] Loss: 1.8247\n",
            "Epoch [7/80], Step [300/500] Loss: 1.5441\n",
            "Epoch [7/80], Step [400/500] Loss: 1.4794\n",
            "Epoch [7/80], Step [500/500] Loss: 1.3617\n",
            "Epoch [8/80], Step [100/500] Loss: 1.5657\n",
            "Epoch [8/80], Step [200/500] Loss: 1.1273\n",
            "Epoch [8/80], Step [300/500] Loss: 1.4259\n",
            "Epoch [8/80], Step [400/500] Loss: 1.4202\n",
            "Epoch [8/80], Step [500/500] Loss: 1.3795\n",
            "Epoch [9/80], Step [100/500] Loss: 1.2724\n",
            "Epoch [9/80], Step [200/500] Loss: 1.4077\n",
            "Epoch [9/80], Step [300/500] Loss: 1.8439\n",
            "Epoch [9/80], Step [400/500] Loss: 1.4568\n",
            "Epoch [9/80], Step [500/500] Loss: 1.4362\n",
            "Epoch [10/80], Step [100/500] Loss: 1.5656\n",
            "Epoch [10/80], Step [200/500] Loss: 1.3448\n",
            "Epoch [10/80], Step [300/500] Loss: 1.2382\n",
            "Epoch [10/80], Step [400/500] Loss: 1.2659\n",
            "Epoch [10/80], Step [500/500] Loss: 0.9971\n",
            "Epoch [11/80], Step [100/500] Loss: 1.2435\n",
            "Epoch [11/80], Step [200/500] Loss: 1.4809\n",
            "Epoch [11/80], Step [300/500] Loss: 1.3267\n",
            "Epoch [11/80], Step [400/500] Loss: 1.2853\n",
            "Epoch [11/80], Step [500/500] Loss: 1.1029\n",
            "Epoch [12/80], Step [100/500] Loss: 0.8752\n",
            "Epoch [12/80], Step [200/500] Loss: 1.0313\n",
            "Epoch [12/80], Step [300/500] Loss: 1.0394\n",
            "Epoch [12/80], Step [400/500] Loss: 1.2860\n",
            "Epoch [12/80], Step [500/500] Loss: 1.2201\n",
            "Epoch [13/80], Step [100/500] Loss: 1.0790\n",
            "Epoch [13/80], Step [200/500] Loss: 1.0833\n",
            "Epoch [13/80], Step [300/500] Loss: 1.3446\n",
            "Epoch [13/80], Step [400/500] Loss: 1.2451\n",
            "Epoch [13/80], Step [500/500] Loss: 1.0876\n",
            "Epoch [14/80], Step [100/500] Loss: 1.0108\n",
            "Epoch [14/80], Step [200/500] Loss: 1.0794\n",
            "Epoch [14/80], Step [300/500] Loss: 1.0835\n",
            "Epoch [14/80], Step [400/500] Loss: 0.9450\n",
            "Epoch [14/80], Step [500/500] Loss: 1.6783\n",
            "Epoch [15/80], Step [100/500] Loss: 1.2334\n",
            "Epoch [15/80], Step [200/500] Loss: 1.2418\n",
            "Epoch [15/80], Step [300/500] Loss: 1.2831\n",
            "Epoch [15/80], Step [400/500] Loss: 1.5343\n",
            "Epoch [15/80], Step [500/500] Loss: 1.4711\n",
            "Epoch [16/80], Step [100/500] Loss: 1.0218\n",
            "Epoch [16/80], Step [200/500] Loss: 0.7263\n",
            "Epoch [16/80], Step [300/500] Loss: 1.1353\n",
            "Epoch [16/80], Step [400/500] Loss: 1.0349\n",
            "Epoch [16/80], Step [500/500] Loss: 1.2354\n",
            "Epoch [17/80], Step [100/500] Loss: 1.0060\n",
            "Epoch [17/80], Step [200/500] Loss: 1.0025\n",
            "Epoch [17/80], Step [300/500] Loss: 1.0865\n",
            "Epoch [17/80], Step [400/500] Loss: 1.1480\n",
            "Epoch [17/80], Step [500/500] Loss: 0.9812\n",
            "Epoch [18/80], Step [100/500] Loss: 0.9207\n",
            "Epoch [18/80], Step [200/500] Loss: 1.2027\n",
            "Epoch [18/80], Step [300/500] Loss: 1.2246\n",
            "Epoch [18/80], Step [400/500] Loss: 1.3253\n",
            "Epoch [18/80], Step [500/500] Loss: 1.1029\n",
            "Epoch [19/80], Step [100/500] Loss: 0.9252\n",
            "Epoch [19/80], Step [200/500] Loss: 0.9692\n",
            "Epoch [19/80], Step [300/500] Loss: 0.8043\n",
            "Epoch [19/80], Step [400/500] Loss: 0.7557\n",
            "Epoch [19/80], Step [500/500] Loss: 0.7628\n",
            "Epoch [20/80], Step [100/500] Loss: 0.8675\n",
            "Epoch [20/80], Step [200/500] Loss: 0.7615\n",
            "Epoch [20/80], Step [300/500] Loss: 0.7176\n",
            "Epoch [20/80], Step [400/500] Loss: 0.8958\n",
            "Epoch [20/80], Step [500/500] Loss: 0.7673\n",
            "Epoch [21/80], Step [100/500] Loss: 0.6734\n",
            "Epoch [21/80], Step [200/500] Loss: 0.8516\n",
            "Epoch [21/80], Step [300/500] Loss: 0.7509\n",
            "Epoch [21/80], Step [400/500] Loss: 0.7458\n",
            "Epoch [21/80], Step [500/500] Loss: 0.5208\n",
            "Epoch [22/80], Step [100/500] Loss: 0.5615\n",
            "Epoch [22/80], Step [200/500] Loss: 0.5873\n",
            "Epoch [22/80], Step [300/500] Loss: 0.5534\n",
            "Epoch [22/80], Step [400/500] Loss: 0.6546\n",
            "Epoch [22/80], Step [500/500] Loss: 0.7602\n",
            "Epoch [23/80], Step [100/500] Loss: 0.6609\n",
            "Epoch [23/80], Step [200/500] Loss: 0.5754\n",
            "Epoch [23/80], Step [300/500] Loss: 0.6142\n",
            "Epoch [23/80], Step [400/500] Loss: 0.6560\n",
            "Epoch [23/80], Step [500/500] Loss: 0.5546\n",
            "Epoch [24/80], Step [100/500] Loss: 0.6822\n",
            "Epoch [24/80], Step [200/500] Loss: 0.5693\n",
            "Epoch [24/80], Step [300/500] Loss: 0.3885\n",
            "Epoch [24/80], Step [400/500] Loss: 0.5055\n",
            "Epoch [24/80], Step [500/500] Loss: 0.5731\n",
            "Epoch [25/80], Step [100/500] Loss: 0.6406\n",
            "Epoch [25/80], Step [200/500] Loss: 0.5981\n",
            "Epoch [25/80], Step [300/500] Loss: 0.4735\n",
            "Epoch [25/80], Step [400/500] Loss: 0.5491\n",
            "Epoch [25/80], Step [500/500] Loss: 0.6402\n",
            "Epoch [26/80], Step [100/500] Loss: 0.4237\n",
            "Epoch [26/80], Step [200/500] Loss: 0.5338\n",
            "Epoch [26/80], Step [300/500] Loss: 0.3303\n",
            "Epoch [26/80], Step [400/500] Loss: 0.4733\n",
            "Epoch [26/80], Step [500/500] Loss: 0.5371\n",
            "Epoch [27/80], Step [100/500] Loss: 0.4499\n",
            "Epoch [27/80], Step [200/500] Loss: 0.5702\n",
            "Epoch [27/80], Step [300/500] Loss: 0.5282\n",
            "Epoch [27/80], Step [400/500] Loss: 0.5899\n",
            "Epoch [27/80], Step [500/500] Loss: 0.6610\n",
            "Epoch [28/80], Step [100/500] Loss: 0.7266\n",
            "Epoch [28/80], Step [200/500] Loss: 0.4547\n",
            "Epoch [28/80], Step [300/500] Loss: 0.5068\n",
            "Epoch [28/80], Step [400/500] Loss: 0.4380\n",
            "Epoch [28/80], Step [500/500] Loss: 0.6880\n",
            "Epoch [29/80], Step [100/500] Loss: 0.5573\n",
            "Epoch [29/80], Step [200/500] Loss: 0.5285\n",
            "Epoch [29/80], Step [300/500] Loss: 0.4957\n",
            "Epoch [29/80], Step [400/500] Loss: 0.5989\n",
            "Epoch [29/80], Step [500/500] Loss: 0.7495\n",
            "Epoch [30/80], Step [100/500] Loss: 0.5510\n",
            "Epoch [30/80], Step [200/500] Loss: 0.5132\n",
            "Epoch [30/80], Step [300/500] Loss: 0.6462\n",
            "Epoch [30/80], Step [400/500] Loss: 0.4284\n",
            "Epoch [30/80], Step [500/500] Loss: 0.8218\n",
            "Epoch [31/80], Step [100/500] Loss: 0.4653\n",
            "Epoch [31/80], Step [200/500] Loss: 0.6193\n",
            "Epoch [31/80], Step [300/500] Loss: 0.5525\n",
            "Epoch [31/80], Step [400/500] Loss: 0.4033\n",
            "Epoch [31/80], Step [500/500] Loss: 0.4519\n",
            "Epoch [32/80], Step [100/500] Loss: 0.6024\n",
            "Epoch [32/80], Step [200/500] Loss: 0.4351\n",
            "Epoch [32/80], Step [300/500] Loss: 0.5159\n",
            "Epoch [32/80], Step [400/500] Loss: 0.5185\n",
            "Epoch [32/80], Step [500/500] Loss: 0.4805\n",
            "Epoch [33/80], Step [100/500] Loss: 0.3473\n",
            "Epoch [33/80], Step [200/500] Loss: 0.4857\n",
            "Epoch [33/80], Step [300/500] Loss: 0.3898\n",
            "Epoch [33/80], Step [400/500] Loss: 0.4610\n",
            "Epoch [33/80], Step [500/500] Loss: 0.4576\n",
            "Epoch [34/80], Step [100/500] Loss: 0.4450\n",
            "Epoch [34/80], Step [200/500] Loss: 0.3429\n",
            "Epoch [34/80], Step [300/500] Loss: 0.5779\n",
            "Epoch [34/80], Step [400/500] Loss: 0.4096\n",
            "Epoch [34/80], Step [500/500] Loss: 0.4496\n",
            "Epoch [35/80], Step [100/500] Loss: 0.4259\n",
            "Epoch [35/80], Step [200/500] Loss: 0.5074\n",
            "Epoch [35/80], Step [300/500] Loss: 0.5335\n",
            "Epoch [35/80], Step [400/500] Loss: 0.4176\n",
            "Epoch [35/80], Step [500/500] Loss: 0.5358\n",
            "Epoch [36/80], Step [100/500] Loss: 0.3396\n",
            "Epoch [36/80], Step [200/500] Loss: 0.4253\n",
            "Epoch [36/80], Step [300/500] Loss: 0.3990\n",
            "Epoch [36/80], Step [400/500] Loss: 0.4997\n",
            "Epoch [36/80], Step [500/500] Loss: 0.4289\n",
            "Epoch [37/80], Step [100/500] Loss: 0.4164\n",
            "Epoch [37/80], Step [200/500] Loss: 0.3286\n",
            "Epoch [37/80], Step [300/500] Loss: 0.5290\n",
            "Epoch [37/80], Step [400/500] Loss: 0.3828\n",
            "Epoch [37/80], Step [500/500] Loss: 0.5088\n",
            "Epoch [38/80], Step [100/500] Loss: 0.5345\n",
            "Epoch [38/80], Step [200/500] Loss: 0.3389\n",
            "Epoch [38/80], Step [300/500] Loss: 0.3492\n",
            "Epoch [38/80], Step [400/500] Loss: 0.6148\n",
            "Epoch [38/80], Step [500/500] Loss: 0.3615\n",
            "Epoch [39/80], Step [100/500] Loss: 0.2631\n",
            "Epoch [39/80], Step [200/500] Loss: 0.3149\n",
            "Epoch [39/80], Step [300/500] Loss: 0.4396\n",
            "Epoch [39/80], Step [400/500] Loss: 0.3931\n",
            "Epoch [39/80], Step [500/500] Loss: 0.3477\n",
            "Epoch [40/80], Step [100/500] Loss: 0.4317\n",
            "Epoch [40/80], Step [200/500] Loss: 0.2542\n",
            "Epoch [40/80], Step [300/500] Loss: 0.3614\n",
            "Epoch [40/80], Step [400/500] Loss: 0.3400\n",
            "Epoch [40/80], Step [500/500] Loss: 0.4119\n",
            "Epoch [41/80], Step [100/500] Loss: 0.2912\n",
            "Epoch [41/80], Step [200/500] Loss: 0.2427\n",
            "Epoch [41/80], Step [300/500] Loss: 0.2723\n",
            "Epoch [41/80], Step [400/500] Loss: 0.3265\n",
            "Epoch [41/80], Step [500/500] Loss: 0.2587\n",
            "Epoch [42/80], Step [100/500] Loss: 0.2800\n",
            "Epoch [42/80], Step [200/500] Loss: 0.1705\n",
            "Epoch [42/80], Step [300/500] Loss: 0.3845\n",
            "Epoch [42/80], Step [400/500] Loss: 0.2976\n",
            "Epoch [42/80], Step [500/500] Loss: 0.3563\n",
            "Epoch [43/80], Step [100/500] Loss: 0.3519\n",
            "Epoch [43/80], Step [200/500] Loss: 0.2498\n",
            "Epoch [43/80], Step [300/500] Loss: 0.3372\n",
            "Epoch [43/80], Step [400/500] Loss: 0.3480\n",
            "Epoch [43/80], Step [500/500] Loss: 0.2863\n",
            "Epoch [44/80], Step [100/500] Loss: 0.2961\n",
            "Epoch [44/80], Step [200/500] Loss: 0.3035\n",
            "Epoch [44/80], Step [300/500] Loss: 0.2922\n",
            "Epoch [44/80], Step [400/500] Loss: 0.2178\n",
            "Epoch [44/80], Step [500/500] Loss: 0.4804\n",
            "Epoch [45/80], Step [100/500] Loss: 0.2390\n",
            "Epoch [45/80], Step [200/500] Loss: 0.3164\n",
            "Epoch [45/80], Step [300/500] Loss: 0.1669\n",
            "Epoch [45/80], Step [400/500] Loss: 0.2213\n",
            "Epoch [45/80], Step [500/500] Loss: 0.3171\n",
            "Epoch [46/80], Step [100/500] Loss: 0.2285\n",
            "Epoch [46/80], Step [200/500] Loss: 0.2548\n",
            "Epoch [46/80], Step [300/500] Loss: 0.1735\n",
            "Epoch [46/80], Step [400/500] Loss: 0.3408\n",
            "Epoch [46/80], Step [500/500] Loss: 0.2453\n",
            "Epoch [47/80], Step [100/500] Loss: 0.3170\n",
            "Epoch [47/80], Step [200/500] Loss: 0.3212\n",
            "Epoch [47/80], Step [300/500] Loss: 0.2579\n",
            "Epoch [47/80], Step [400/500] Loss: 0.3724\n",
            "Epoch [47/80], Step [500/500] Loss: 0.2664\n",
            "Epoch [48/80], Step [100/500] Loss: 0.1964\n",
            "Epoch [48/80], Step [200/500] Loss: 0.2189\n",
            "Epoch [48/80], Step [300/500] Loss: 0.1989\n",
            "Epoch [48/80], Step [400/500] Loss: 0.2084\n",
            "Epoch [48/80], Step [500/500] Loss: 0.2552\n",
            "Epoch [49/80], Step [100/500] Loss: 0.2487\n",
            "Epoch [49/80], Step [200/500] Loss: 0.2270\n",
            "Epoch [49/80], Step [300/500] Loss: 0.1913\n",
            "Epoch [49/80], Step [400/500] Loss: 0.1804\n",
            "Epoch [49/80], Step [500/500] Loss: 0.1934\n",
            "Epoch [50/80], Step [100/500] Loss: 0.2108\n",
            "Epoch [50/80], Step [200/500] Loss: 0.1886\n",
            "Epoch [50/80], Step [300/500] Loss: 0.2375\n",
            "Epoch [50/80], Step [400/500] Loss: 0.2921\n",
            "Epoch [50/80], Step [500/500] Loss: 0.1229\n",
            "Epoch [51/80], Step [100/500] Loss: 0.2172\n",
            "Epoch [51/80], Step [200/500] Loss: 0.2772\n",
            "Epoch [51/80], Step [300/500] Loss: 0.1603\n",
            "Epoch [51/80], Step [400/500] Loss: 0.1837\n",
            "Epoch [51/80], Step [500/500] Loss: 0.4494\n",
            "Epoch [52/80], Step [100/500] Loss: 0.3277\n",
            "Epoch [52/80], Step [200/500] Loss: 0.1711\n",
            "Epoch [52/80], Step [300/500] Loss: 0.3179\n",
            "Epoch [52/80], Step [400/500] Loss: 0.2118\n",
            "Epoch [52/80], Step [500/500] Loss: 0.3117\n",
            "Epoch [53/80], Step [100/500] Loss: 0.2496\n",
            "Epoch [53/80], Step [200/500] Loss: 0.2461\n",
            "Epoch [53/80], Step [300/500] Loss: 0.2623\n",
            "Epoch [53/80], Step [400/500] Loss: 0.1761\n",
            "Epoch [53/80], Step [500/500] Loss: 0.3714\n",
            "Epoch [54/80], Step [100/500] Loss: 0.1963\n",
            "Epoch [54/80], Step [200/500] Loss: 0.1711\n",
            "Epoch [54/80], Step [300/500] Loss: 0.1783\n",
            "Epoch [54/80], Step [400/500] Loss: 0.1955\n",
            "Epoch [54/80], Step [500/500] Loss: 0.2261\n",
            "Epoch [55/80], Step [100/500] Loss: 0.1252\n",
            "Epoch [55/80], Step [200/500] Loss: 0.1523\n",
            "Epoch [55/80], Step [300/500] Loss: 0.1626\n",
            "Epoch [55/80], Step [400/500] Loss: 0.2279\n",
            "Epoch [55/80], Step [500/500] Loss: 0.2037\n",
            "Epoch [56/80], Step [100/500] Loss: 0.3389\n",
            "Epoch [56/80], Step [200/500] Loss: 0.3261\n",
            "Epoch [56/80], Step [300/500] Loss: 0.1347\n",
            "Epoch [56/80], Step [400/500] Loss: 0.2310\n",
            "Epoch [56/80], Step [500/500] Loss: 0.2232\n",
            "Epoch [57/80], Step [100/500] Loss: 0.1721\n",
            "Epoch [57/80], Step [200/500] Loss: 0.1805\n",
            "Epoch [57/80], Step [300/500] Loss: 0.2568\n",
            "Epoch [57/80], Step [400/500] Loss: 0.1943\n",
            "Epoch [57/80], Step [500/500] Loss: 0.2902\n",
            "Epoch [58/80], Step [100/500] Loss: 0.1424\n",
            "Epoch [58/80], Step [200/500] Loss: 0.1770\n",
            "Epoch [58/80], Step [300/500] Loss: 0.1583\n",
            "Epoch [58/80], Step [400/500] Loss: 0.2859\n",
            "Epoch [58/80], Step [500/500] Loss: 0.1678\n",
            "Epoch [59/80], Step [100/500] Loss: 0.2546\n",
            "Epoch [59/80], Step [200/500] Loss: 0.1362\n",
            "Epoch [59/80], Step [300/500] Loss: 0.2857\n",
            "Epoch [59/80], Step [400/500] Loss: 0.2085\n",
            "Epoch [59/80], Step [500/500] Loss: 0.1802\n",
            "Epoch [60/80], Step [100/500] Loss: 0.2187\n",
            "Epoch [60/80], Step [200/500] Loss: 0.1578\n",
            "Epoch [60/80], Step [300/500] Loss: 0.1556\n",
            "Epoch [60/80], Step [400/500] Loss: 0.1845\n",
            "Epoch [60/80], Step [500/500] Loss: 0.1872\n",
            "Epoch [61/80], Step [100/500] Loss: 0.1343\n",
            "Epoch [61/80], Step [200/500] Loss: 0.1301\n",
            "Epoch [61/80], Step [300/500] Loss: 0.0942\n",
            "Epoch [61/80], Step [400/500] Loss: 0.1152\n",
            "Epoch [61/80], Step [500/500] Loss: 0.1567\n",
            "Epoch [62/80], Step [100/500] Loss: 0.1856\n",
            "Epoch [62/80], Step [200/500] Loss: 0.1320\n",
            "Epoch [62/80], Step [300/500] Loss: 0.2083\n",
            "Epoch [62/80], Step [400/500] Loss: 0.1787\n",
            "Epoch [62/80], Step [500/500] Loss: 0.1036\n",
            "Epoch [63/80], Step [100/500] Loss: 0.0892\n",
            "Epoch [63/80], Step [200/500] Loss: 0.1739\n",
            "Epoch [63/80], Step [300/500] Loss: 0.3395\n",
            "Epoch [63/80], Step [400/500] Loss: 0.1177\n",
            "Epoch [63/80], Step [500/500] Loss: 0.1589\n",
            "Epoch [64/80], Step [100/500] Loss: 0.1337\n",
            "Epoch [64/80], Step [200/500] Loss: 0.0871\n",
            "Epoch [64/80], Step [300/500] Loss: 0.1826\n",
            "Epoch [64/80], Step [400/500] Loss: 0.1572\n",
            "Epoch [64/80], Step [500/500] Loss: 0.1828\n",
            "Epoch [65/80], Step [100/500] Loss: 0.1055\n",
            "Epoch [65/80], Step [200/500] Loss: 0.1867\n",
            "Epoch [65/80], Step [300/500] Loss: 0.1463\n",
            "Epoch [65/80], Step [400/500] Loss: 0.1349\n",
            "Epoch [65/80], Step [500/500] Loss: 0.1920\n",
            "Epoch [66/80], Step [100/500] Loss: 0.1494\n",
            "Epoch [66/80], Step [200/500] Loss: 0.0605\n",
            "Epoch [66/80], Step [300/500] Loss: 0.1549\n",
            "Epoch [66/80], Step [400/500] Loss: 0.0952\n",
            "Epoch [66/80], Step [500/500] Loss: 0.1792\n",
            "Epoch [67/80], Step [100/500] Loss: 0.0947\n",
            "Epoch [67/80], Step [200/500] Loss: 0.2228\n",
            "Epoch [67/80], Step [300/500] Loss: 0.1752\n",
            "Epoch [67/80], Step [400/500] Loss: 0.1259\n",
            "Epoch [67/80], Step [500/500] Loss: 0.1005\n",
            "Epoch [68/80], Step [100/500] Loss: 0.0821\n",
            "Epoch [68/80], Step [200/500] Loss: 0.1639\n",
            "Epoch [68/80], Step [300/500] Loss: 0.0876\n",
            "Epoch [68/80], Step [400/500] Loss: 0.0447\n",
            "Epoch [68/80], Step [500/500] Loss: 0.1300\n",
            "Epoch [69/80], Step [100/500] Loss: 0.1390\n",
            "Epoch [69/80], Step [200/500] Loss: 0.1554\n",
            "Epoch [69/80], Step [300/500] Loss: 0.2223\n",
            "Epoch [69/80], Step [400/500] Loss: 0.1497\n",
            "Epoch [69/80], Step [500/500] Loss: 0.1233\n",
            "Epoch [70/80], Step [100/500] Loss: 0.1399\n",
            "Epoch [70/80], Step [200/500] Loss: 0.1803\n",
            "Epoch [70/80], Step [300/500] Loss: 0.2314\n",
            "Epoch [70/80], Step [400/500] Loss: 0.0736\n",
            "Epoch [70/80], Step [500/500] Loss: 0.1406\n",
            "Epoch [71/80], Step [100/500] Loss: 0.0898\n",
            "Epoch [71/80], Step [200/500] Loss: 0.1694\n",
            "Epoch [71/80], Step [300/500] Loss: 0.1134\n",
            "Epoch [71/80], Step [400/500] Loss: 0.1118\n",
            "Epoch [71/80], Step [500/500] Loss: 0.0756\n",
            "Epoch [72/80], Step [100/500] Loss: 0.0696\n",
            "Epoch [72/80], Step [200/500] Loss: 0.0757\n",
            "Epoch [72/80], Step [300/500] Loss: 0.0978\n",
            "Epoch [72/80], Step [400/500] Loss: 0.0714\n",
            "Epoch [72/80], Step [500/500] Loss: 0.1531\n",
            "Epoch [73/80], Step [100/500] Loss: 0.0547\n",
            "Epoch [73/80], Step [200/500] Loss: 0.1024\n",
            "Epoch [73/80], Step [300/500] Loss: 0.1168\n",
            "Epoch [73/80], Step [400/500] Loss: 0.1907\n",
            "Epoch [73/80], Step [500/500] Loss: 0.1144\n",
            "Epoch [74/80], Step [100/500] Loss: 0.1483\n",
            "Epoch [74/80], Step [200/500] Loss: 0.0574\n",
            "Epoch [74/80], Step [300/500] Loss: 0.1813\n",
            "Epoch [74/80], Step [400/500] Loss: 0.0865\n",
            "Epoch [74/80], Step [500/500] Loss: 0.0944\n",
            "Epoch [75/80], Step [100/500] Loss: 0.0807\n",
            "Epoch [75/80], Step [200/500] Loss: 0.1898\n",
            "Epoch [75/80], Step [300/500] Loss: 0.0947\n",
            "Epoch [75/80], Step [400/500] Loss: 0.1350\n",
            "Epoch [75/80], Step [500/500] Loss: 0.1530\n",
            "Epoch [76/80], Step [100/500] Loss: 0.2200\n",
            "Epoch [76/80], Step [200/500] Loss: 0.0863\n",
            "Epoch [76/80], Step [300/500] Loss: 0.1124\n",
            "Epoch [76/80], Step [400/500] Loss: 0.1914\n",
            "Epoch [76/80], Step [500/500] Loss: 0.1201\n",
            "Epoch [77/80], Step [100/500] Loss: 0.1526\n",
            "Epoch [77/80], Step [200/500] Loss: 0.1004\n",
            "Epoch [77/80], Step [300/500] Loss: 0.1171\n",
            "Epoch [77/80], Step [400/500] Loss: 0.0998\n",
            "Epoch [77/80], Step [500/500] Loss: 0.1299\n",
            "Epoch [78/80], Step [100/500] Loss: 0.0445\n",
            "Epoch [78/80], Step [200/500] Loss: 0.0977\n",
            "Epoch [78/80], Step [300/500] Loss: 0.1352\n",
            "Epoch [78/80], Step [400/500] Loss: 0.1492\n",
            "Epoch [78/80], Step [500/500] Loss: 0.0961\n",
            "Epoch [79/80], Step [100/500] Loss: 0.0563\n",
            "Epoch [79/80], Step [200/500] Loss: 0.1912\n",
            "Epoch [79/80], Step [300/500] Loss: 0.0709\n",
            "Epoch [79/80], Step [400/500] Loss: 0.0793\n",
            "Epoch [79/80], Step [500/500] Loss: 0.0855\n",
            "Epoch [80/80], Step [100/500] Loss: 0.0840\n",
            "Epoch [80/80], Step [200/500] Loss: 0.1807\n",
            "Epoch [80/80], Step [300/500] Loss: 0.1531\n",
            "Epoch [80/80], Step [400/500] Loss: 0.0947\n",
            "Epoch [80/80], Step [500/500] Loss: 0.1615\n"
          ]
        }
      ],
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# For updating learning rate\n",
        "def update_lr(optimizer, lr):    \n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "# Training the model\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Decay learning rate\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        curr_lr /= 3\n",
        "        update_lr(optimizer, curr_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGkeoaBE3Kd"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUD2lrO-E55d",
        "outputId": "92d37537-0366-46f6-8974-1214e70270fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 85.57 %\n"
          ]
        }
      ],
      "source": [
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'resnet.ckpt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ResNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyODuzgC0agHRLfICShMYn8I",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}